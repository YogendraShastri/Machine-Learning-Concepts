# Machine-Learning-Concepts
Here's a Machine Learning Topics Tree organized in a hierarchical structure so you can clearly see which topic belongs where and we gonna cover them all.

```bash
Machine Learning
â”œâ”€â”€ 1. Types of Learning
â”‚   â”œâ”€â”€ 1.1 Supervised Learning
â”‚   â”‚   â”œâ”€â”€ Regression
â”‚   â”‚   â”‚   â”œâ”€â”€ Linear Regression 
â”‚   â”‚   â”‚   â”œâ”€â”€ Polynomial Regression 
â”‚   â”‚   â”‚   â”œâ”€â”€ Ridge / Lasso Regression
â”‚   â”‚   â”œâ”€â”€ Classification
â”‚   â”‚   â”‚   â”œâ”€â”€ Logistic Regression 
â”‚   â”‚   â”‚   â”œâ”€â”€ K-Nearest Neighbors (KNN) 
â”‚   â”‚   â”‚   â”œâ”€â”€ Support Vector Machine (SVM) 
â”‚   â”‚   â”‚   â”œâ”€â”€ Decision Trees 
â”‚   â”‚   â”‚   â”œâ”€â”€ Naive Bayes 
â”‚
â”‚   â”œâ”€â”€ 1.2 Unsupervised Learning 
â”‚   â”‚   â”œâ”€â”€ Clustering
â”‚   â”‚   â”‚   â”œâ”€â”€ K-Means 
â”‚   â”‚   â”‚   â”œâ”€â”€ Hierarchical Clustering 
â”‚   â”‚   â”œâ”€â”€ Dimensionality Reduction
â”‚   â”‚   â”‚   â”œâ”€â”€ PCA (Principal Component Analysis) 
â”‚   â”‚   â”‚   â”œâ”€â”€ t-SNE 
â”‚
â”‚   â”œâ”€â”€ 1.3 Reinforcement Learning 
â”‚       â”œâ”€â”€ Q-Learning
â”‚       â”œâ”€â”€ Deep Q-Networks (DQN)

# Math Prerequisite

â”œâ”€â”€ 2. Math Foundations
â”‚   â”œâ”€â”€ Linear Algebra ğŸ”œ
â”‚   â”œâ”€â”€ Calculus (Gradient Descent) 
â”‚   â”œâ”€â”€ Probability & Statistics 
â”‚   â”œâ”€â”€ Optimization
â”‚       â”œâ”€â”€ Cost Functions
â”‚       â”‚   â”œâ”€â”€ MSE 
â”‚       â”‚   â”œâ”€â”€ Cross-Entropy Loss 
â”‚       â”œâ”€â”€ Gradient Descent 

# Training and Preprocessing

â”œâ”€â”€ 3. Model Development
â”‚   â”œâ”€â”€ Data Splitting
â”‚   â”‚   â”œâ”€â”€ Train/Test Split 
â”‚   â”‚   â”œâ”€â”€ Cross-Validation 
â”‚   â”œâ”€â”€ Data Preprocessing
â”‚   â”‚   â”œâ”€â”€ Scaling / Normalization 
â”‚   â”‚   â”œâ”€â”€ Encoding Categorical Data 
â”‚   â”‚   â”œâ”€â”€ Handling Missing Values 

# Model Evaluation

â”œâ”€â”€ 4. Model Evaluation
â”‚   â”œâ”€â”€ Regression Metrics
â”‚   â”‚   â”œâ”€â”€ Mean Squared Error (MSE) 
â”‚   â”‚   â”œâ”€â”€ RÂ² Score 
â”‚   â”œâ”€â”€ Classification Metrics
â”‚   â”‚   â”œâ”€â”€ Accuracy 
â”‚   â”‚   â”œâ”€â”€ Precision 
â”‚   â”‚   â”œâ”€â”€ Recall 
â”‚   â”‚   â”œâ”€â”€ F1 Score 
â”‚   â”‚   â”œâ”€â”€ Confusion Matrix

```
While learning about all this machine learning concepts and models, I find myself hard to remember that how i imported certain model last time, so here is the tree to remember the sklearn models. Here's a well-organized Tree View of sklearn's models and where they live inside the library, so you can see which models belong to which category and module (e.g., classification, regression, clustering, etc.).

```bash
sklearn
â”œâ”€â”€ model_selection
â”‚   â”œâ”€â”€ train_test_split 
â”‚   â”œâ”€â”€ cross_val_score
â”‚   â”œâ”€â”€ GridSearchCV
â”‚   â”œâ”€â”€ StratifiedKFold
â”‚
â”œâ”€â”€ linear_model
â”‚   â”œâ”€â”€ LinearRegression 
â”‚   â”œâ”€â”€ LogisticRegression 
â”‚   â”œâ”€â”€ Ridge
â”‚   â”œâ”€â”€ Lasso 
â”‚   â”œâ”€â”€ ElasticNet 
â”‚   â”œâ”€â”€ SGDClassifier 
â”‚   â”œâ”€â”€ SGDRegressor
â”‚
â”œâ”€â”€ ensemble
â”‚   â”œâ”€â”€ RandomForestClassifier
â”‚   â”œâ”€â”€ RandomForestRegressor
â”‚   â”œâ”€â”€ GradientBoostingClassifier
â”‚   â”œâ”€â”€ GradientBoostingRegressor
â”‚   â”œâ”€â”€ AdaBoostClassifier
â”‚   â”œâ”€â”€ AdaBoostRegressor
â”‚   â”œâ”€â”€ BaggingClassifier
â”‚   â”œâ”€â”€ VotingClassifier
â”‚
â”œâ”€â”€ tree
â”‚   â”œâ”€â”€ DecisionTreeClassifier
â”‚   â”œâ”€â”€ DecisionTreeRegressor
â”‚
â”œâ”€â”€ neighbors
â”‚   â”œâ”€â”€ KNeighborsClassifier
â”‚   â”œâ”€â”€ KNeighborsRegressor
â”‚
â”œâ”€â”€ svm
â”‚   â”œâ”€â”€ SVC (Support Vector Classifier)
â”‚   â”œâ”€â”€ SVR (Support Vector Regressor)
â”‚   â”œâ”€â”€ LinearSVC
â”‚   â”œâ”€â”€ NuSVC
â”‚
â”œâ”€â”€ naive_bayes
â”‚   â”œâ”€â”€ GaussianNB
â”‚   â”œâ”€â”€ MultinomialNB
â”‚   â”œâ”€â”€ BernoulliNB
â”‚
â”œâ”€â”€ cluster
â”‚   â”œâ”€â”€ KMeans
â”‚   â”œâ”€â”€ DBSCAN 
â”‚   â”œâ”€â”€ AgglomerativeClustering
â”‚
â”œâ”€â”€ decomposition
â”‚   â”œâ”€â”€ PCA (Principal Component Analysis)
â”‚   â”œâ”€â”€ TruncatedSVD
â”‚
â”‚
â”œâ”€â”€ metrics
â”‚   â”œâ”€â”€ accuracy_score
â”‚   â”œâ”€â”€ mean_squared_error
â”‚   â”œâ”€â”€ r2_score
â”‚   â”œâ”€â”€ confusion_matrix
â”‚   â”œâ”€â”€ precision_score
â”‚   â”œâ”€â”€ recall_score
â”‚   â”œâ”€â”€ f1_score
```

## What is Machine Learning?
- **Machine Learning (ML)** is a branch of **Artificial Intelligence (AI)** that focuses on building systems that can automatically learn and improve from experience without being **explicitly programmed** for every task.
- For Example have a module which classcify email to **SPAM** and **HAM**. So Instead of explicitly programming spam rules, we let the machine learn patterns from real-world email data. It then applies that knowledge to classify new emailsâ€”even if the spammer slightly changes their tricks.
- â€œ**Machine learning** is the field of study that gives computers the ability to learn without being explicitly programmed.â€ â€” **Arthur Samuel, 1959**.

  <img width="350" height="346" alt="image" src="https://github.com/user-attachments/assets/ab777ec1-2b68-4660-850b-d63e752ab309" />


### Types of Machine Learning:
1. Supervised Learning
2. Unsupervised Learning
3. Reinforcement Learning

### Supervised Learning
- **Supervised Learning** is like learning with a teacher. The teacher not only tells you what each thing is, but also corrects you when you're wrong. In this analogy, the teacher acts as the **supervisor**.
- So, in **supervised learning**, the machine is trained on labeled dataâ€”which means each input comes with the correct output (label). The model learns by comparing its predictions to the actual answers, just like a student learns by getting feedback from the teacher.
- Supervised Learning can be broadly divided into two main types :
  1. **Regression**
  2. **Classification**
 
### Unsupervised Learning
- **Unsupervised learning** is like learning without a teacher. You're given a pile of information, but no labels or instructions about what it is or what it means. The system has to find patterns and relationships in the data all by itself.
- In this type of learning, the machine is not told what to predict, but rather it tries to make sense of the dataâ€”by grouping similar things together or reducing complexity.
- Common Types of Unsupervised Learning :
  1. **Clustering**
  2. **Dimensionality Reduction**
 
### Reinforcement Learning
- **Reinforcement Learning** is like learning through **experience and feedbackâ€”just** like training a dog or learning to play a video game.
- Thereâ€™s no teacher telling you the exact answer, but you get **rewards or penalties** based on what you do.
- The machine interacts with an environment, takes actions, and learns by trial and error to **maximize the total reward over time**.
- **Examples** : AI playing chess, Go, or video games like Atari or Dota.

## Machine Learning Topics:
lets learn some important topics of machine learning, we might not cover all the topics, but will try to cover those you must know.

### Linear Regression & Polynomial Regression** :
[**Use this repo**](https://github.com/YogendraShastri/Must-Learn-Regressions-Before-Deep-Learning)

### Ridge / Lasso Regression OR (L1 & L2 regularization)
- Regularization prevents from overfitting and underfitting problem.
- L1 and L2 regularization are methods to avoid overfitting in machine learning models like linear regression.
- They add a penalty to the model to stop it from relying too much on any one feature (by keeping the weights small).

[**Under Fit Vs Best Fit Vs Over Fit**](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)

<img width="606" height="382" alt="image" src="https://github.com/user-attachments/assets/136b26dc-d903-4b44-ab38-cd464a4ccf23" />

#### L1 Regularization (Lasso)
- Adds absolute values of the coefficients to the loss:
- Here Original Loss is MSE (Mean Squared Error) for Regression & cross entropy for Classification.
- L1 Regularization, also known as Lasso Regularization.

<img width="250" height="86" alt="image" src="https://github.com/user-attachments/assets/a25e204b-6cc0-49e5-ad6d-e9651a135a5b" />


#### L2 Regularization (Ridge)
- Adds squared values of the coefficients to the loss.
- L2 Relularization is also known as Ridge.

<img width="250" height="84" alt="image" src="https://github.com/user-attachments/assets/bd13b63b-3709-413d-989a-b0b941f34a4b" />

**Notebook** : []()
